---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/postgresql.cnpg.io/cluster_v1.json
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: postgres16
spec:
  instances: 1
  imageName: ghcr.io/tensorchord/cloudnative-pgvecto.rs:16.4
  # imageName: ghcr.io/cloudnative-pg/postgresql:16.1-17
  primaryUpdateStrategy: unsupervised
  storage:
    size: 100Gi
    storageClass: longhorn-fast
  superuserSecret:
    name: cloudnative-pg-secret
  enableSuperuserAccess: true
  postgresql:
    parameters:
      max_connections: "600"
      max_slot_wal_keep_size: 10GB
      shared_buffers: 512MB
    enableAlterSystem: true
    shared_preload_libraries:
      - "vectors.so"
  resources:
    requests:
      cpu: 100m
    limits:
      memory: 4Gi
  monitoring:
    enablePodMonitor: true
    # Ref: https://github.com/cloudnative-pg/cloudnative-pg/issues/2501
    podMonitorMetricRelabelings:
      - { sourceLabels: ["cluster"], targetLabel: cnpg_cluster, action: replace }
      - { regex: cluster, action: labeldrop }
  backup:
    retentionPolicy: 30d
    barmanObjectStore: &barmanObjectStore
      data:
        compression: bzip2
      wal:
        compression: bzip2
        maxParallel: 8
      destinationPath: s3://cloudnative-pg/
      endpointURL: http://s3.casa
      # Note: serverName version needs to be inclemented
      # when recovering from an existing cnpg cluster
      serverName: &currentCluster postgres16-v3
      s3Credentials:
        accessKeyId:
          name: cloudnative-pg-secret
          key: aws-access-key-id
        secretAccessKey:
          name: cloudnative-pg-secret
          key: aws-secret-access-key
  bootstrap:
    recovery:
      # Recovery object store containing WAL archive and base backups
      source: &previousCluster postgres16-v2
  externalClusters:
    - name: *previousCluster
      barmanObjectStore:
        <<: *barmanObjectStore
        serverName: *previousCluster
